import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import os
import matplotlib.pyplot as plt
import json
from typing import List

# å¯¼å…¥å„æ¨¡å—
from plsr_model import PLSRSpectralModel, train_calibration_model
from spectral_preprocessing import SpectralPreprocessor, load_spectral_data_from_csv, resample_to_reference, evaluate_resampling_reliability
from element_prediction_pipeline import ElementPredictionPipeline, load_element_data
from evaluation_visualization import create_timestamp_directory, plot_results, plot_performance_comparison, plot_prediction_scatter_comparison
from preprocessing_visualization import visualize_complete_preprocessing_pipeline, visualize_preprocessing_step, visualize_resampling_quality, visualize_data_alignment

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

def load_config(config_path="config.json"):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ {config_path}ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨ã€‚")
        return None
    
    with open(config_path, 'r', encoding='utf-8') as f:
        content = f.read()
    content = ""
    # å°è¯•å¤šç§ç¼–ç è¯»å–ï¼Œä¼˜å…ˆ utf-8-sig ä»¥å¤„ç† BOM (Windows å¸¸è§é—®é¢˜)
    for encoding in ['utf-8-sig', 'utf-8', 'gbk']:
        try:
            with open(config_path, 'r', encoding=encoding) as f:
                content = f.read()
            break
        except UnicodeDecodeError:
            continue
            
    if not content:
        print(f"âŒ é”™è¯¯: æ— æ³•è¯»å–é…ç½®æ–‡ä»¶ {config_path} (å°è¯•äº† utf-8, gbk)")
        return None

    try:
        return json.loads(content)
    except json.JSONDecodeError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶ JSON è§£æå¤±è´¥: {e}")
        # å°è¯•å®šä½é”™è¯¯è¡Œ
        lines = content.split('\n')
        if 0 <= e.lineno - 1 < len(lines):
            print(f"   é”™è¯¯ä½ç½®: ç¬¬ {e.lineno} è¡Œé™„è¿‘")
            print(f"   >> {lines[e.lineno - 1].strip()}")
        return None

def align_element_data(element_data: pd.DataFrame, sample_ids: List[str]) -> pd.DataFrame:
    """
    æ ¸å¿ƒä¿®å¤ï¼šæ ¹æ® sample_ids å¯¹ element_data è¿›è¡Œå¼ºåˆ¶å¯¹é½
    é˜²æ­¢å›  Excel é¡ºåºä¸æ–‡ä»¶åé¡ºåºä¸ä¸€è‡´å¯¼è‡´ X-Y é”™ä½
    """
    # 1. å¯»æ‰¾ ID åˆ— (è‡ªåŠ¨è¯†åˆ«)
    id_col = None
    # å¸¸è§çš„ ID åˆ—åå€™é€‰
    candidates = ['sample', 'sample_id', 'id', 'no', 'name', 'ç¼–å·', 'æ ·å“åç§°', 'æ ·å“ç¼–å·', 'index', 'åºå·']
    
    # ç­–ç•¥A: åŒ¹é…åˆ—å
    for col in element_data.columns:
        if str(col).strip().lower() in candidates:
            id_col = col
            break
            
    # ç­–ç•¥B: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæ£€æŸ¥åˆ—å€¼ä¸ sample_ids çš„é‡å åº¦
    if not id_col:
        for col in element_data.columns:
            try:
                col_values = element_data[col].astype(str).str.strip().values
                overlap = sum(1 for sid in sample_ids if str(sid).strip() in col_values)
                if overlap / len(sample_ids) > 0.5: # è¶…è¿‡50%åŒ¹é…
                    id_col = col
                    break
            except Exception:
                continue
    
    if id_col:
        print(f"   [Data Alignment] è‡ªåŠ¨è¯†åˆ«æ ·å“IDåˆ—: '{id_col}'")
        # åˆ›å»ºå‰¯æœ¬ä»¥å…ä¿®æ”¹åŸå§‹æ•°æ®
        df_aligned = element_data.copy()
        # ç»Ÿä¸€è½¬ä¸ºå­—ç¬¦ä¸²å»ç©ºæ ¼
        df_aligned[id_col] = df_aligned[id_col].astype(str).str.strip()
        
        # è®¾ä¸ºç´¢å¼•å¹¶æŒ‰ sample_ids é‡æ’
        df_aligned = df_aligned.set_index(id_col)
        
        # å…³é”®æ­¥éª¤ï¼šreindex ä¼šæŒ‰ç…§ sample_ids çš„é¡ºåºé‡æ’æ•°æ®
        # å¦‚æœæŸä¸ª sample_id åœ¨ Excel ä¸­ä¸å­˜åœ¨ï¼Œå¯¹åº”è¡Œä¼šå˜æˆ NaN (åç»­ä¼šè¢«è¿‡æ»¤)
        # ç¡®ä¿ sample_ids ä¹Ÿæ˜¯å­—ç¬¦ä¸²æ ¼å¼
        sample_ids_str = [str(s).strip() for s in sample_ids]
        df_aligned = df_aligned.reindex(sample_ids_str)
        
        # æ£€æŸ¥ç¼ºå¤±æƒ…å†µ
        missing_count = df_aligned.isnull().all(axis=1).sum()
        if missing_count > 0:
            print(f"   âš ï¸ è­¦å‘Š: å¯¹é½åæœ‰ {missing_count} ä¸ªæ ·å“åœ¨å…ƒç´ è¡¨ä¸­æœªæ‰¾åˆ°æ•°æ® (å°†è¢«è·³è¿‡)")
        
        # é‡ç½®ç´¢å¼•ï¼Œä¿æŒ DataFrame ç»“æ„
        return df_aligned.reset_index()
    else:
        print("   âš ï¸ ä¸¥é‡è­¦å‘Š: æœªèƒ½è‡ªåŠ¨è¯†åˆ«æ ·å“IDåˆ—ï¼å‡è®¾å…ƒç´ è¡¨é¡ºåºä¸å…‰è°±æ–‡ä»¶é¡ºåºä¸€è‡´ã€‚")
        print("      (å¦‚æœ R2 å¾ˆä½ï¼Œè¯·æ£€æŸ¥ Excel ç¬¬ä¸€åˆ—æ˜¯å¦ä¸ºæ ·å“ç¼–å·ï¼Œä¸”ä¸æ–‡ä»¶åä¸€è‡´)")
        if len(element_data) != len(sample_ids):
            print(f"      æ³¨æ„: å…ƒç´ è¡¨è¡Œæ•° ({len(element_data)}) ä¸ æ ·å“æ•° ({len(sample_ids)}) ä¸ä¸€è‡´ï¼Œæå¤§æ¦‚ç‡é”™ä½ï¼")
        return element_data

def plot_component_counts(res_lq, res_calib, res_hq, timestamp_dir, res_calib_self=None):
    """
    ç»˜åˆ¶å„æ¨¡å¼ä¸‹å„å…ƒç´ çš„ä¸»æˆåˆ†æ•°å¯¹æ¯”å›¾
    """
    if not timestamp_dir: return
    
    save_dir = os.path.join(timestamp_dir, "model_analysis")
    os.makedirs(save_dir, exist_ok=True)
    
    # 1. æ±‡æ€»æ•°æ®
    models = [('LQ-only', res_lq), ('Calib-Spec', res_calib), ('HQ-only', res_hq)]
    if res_calib_self:
        models.append(('Calib-Self', res_calib_self))
        
    all_elements = set()
    for _, res in models:
        if res:
            all_elements.update(res.keys())
    
    sorted_elements = sorted(list(all_elements))
    
    # å‡†å¤‡ç»˜å›¾æ•°æ®
    plot_data = {elem: [] for elem in sorted_elements}
    
    for name, res in models:
        for elem in sorted_elements:
            if res and elem in res:
                n = res[elem].get('n_components', 0)
                plot_data[elem].append(n)
            else:
                plot_data[elem].append(0)
                
    # --- å›¾1: ç»¼åˆå¯¹æ¯”å›¾ (Grouped Bar Chart) ---
    x = np.arange(len(sorted_elements))
    total_width = 0.8
    n_models = len(models)
    width = total_width / n_models
    
    plt.figure(figsize=(max(12, len(sorted_elements)*0.8), 6), dpi=300)
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    
    for i, (name, _) in enumerate(models):
        vals = [plot_data[elem][i] for elem in sorted_elements]
        bar_x = x - (total_width / 2) + (i * width) + (width / 2)
        plt.bar(bar_x, vals, width, label=name, alpha=0.8, color=colors[i % len(colors)])
        
    plt.xlabel('Elements')
    plt.ylabel('Number of Components')
    plt.title('Optimal Components by Element and Model')
    plt.xticks(x, sorted_elements, rotation=45)
    plt.legend()
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, "components_comparison_all.png"))
    plt.close()
    
    # --- å›¾2-5: å„æ¨¡å¼å•ç‹¬å›¾ ---
    for i, (name, res) in enumerate(models):
        if not res: continue
        
        elems = []
        comps = []
        for e in sorted_elements:
            if e in res:
                elems.append(e)
                comps.append(res[e].get('n_components', 0))
        
        if not elems: continue
        
        plt.figure(figsize=(max(10, len(elems)*0.6), 5), dpi=300)
        bars = plt.bar(elems, comps, color=colors[i % len(colors)], edgecolor='black', alpha=0.7)
        plt.xlabel('Elements')
        plt.ylabel('Number of Components')
        plt.title(f'Optimal Components - {name}')
        plt.xticks(rotation=45)
        plt.grid(axis='y', alpha=0.3)
        
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                     f'{int(height)}',
                     ha='center', va='bottom', fontsize=9)
                     
        plt.tight_layout()
        safe_name = name.replace(" ", "_").replace("-", "_").replace("(", "").replace(")", "")
        plt.savefig(os.path.join(save_dir, f"components_{safe_name}.png"))
        plt.close()
        
    print(f"   [Plot] ä¸»æˆåˆ†æ•°å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {save_dir}")

def plot_cv_curves(res_lq, res_calib, res_hq, timestamp_dir, res_calib_self=None):
    """
    ç”Ÿæˆå„æ¨¡å¼ä¸‹å„å…ƒç´ çš„ CV å¯»ä¼˜æ›²çº¿ (RMSE vs Components)
    1. ç”Ÿæˆåˆ†é¢å›¾ (Grid Plot): æ¯ä¸ªå…ƒç´ ä¸€å¼ å­å›¾ï¼Œæ˜¾ç¤ºåŸå§‹ RMSE å€¼
    2. ç”Ÿæˆç»¼åˆå›¾ (Combined Plot): æ‰€æœ‰å…ƒç´ å½’ä¸€åŒ–åç”»åœ¨åŒä¸€å¼ å›¾ï¼Œä¾¿äºæ¯”è¾ƒè¶‹åŠ¿
    """
    if not timestamp_dir: return
    
    save_dir = os.path.join(timestamp_dir, "model_analysis", "cv_curves")
    os.makedirs(save_dir, exist_ok=True)
    
    models = [('LQ-only', res_lq), ('Calib-Spec', res_calib), ('HQ-only', res_hq)]
    if res_calib_self:
        models.append(('Calib-Self', res_calib_self))
        
    count = 0
    for mode_name, results in models:
        if not results: continue
        
        elements = sorted([e for e in results.keys() if 'cv_history' in results[e]])
        if not elements: continue
        
        # --- 1. åŸå§‹åˆ†é¢å›¾ (Grid Plot) ---
        n_elems = len(elements)
        cols = 4
        rows = (n_elems + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(20, 4 * rows), dpi=200)
        axes = axes.flatten()
        
        for i, elem in enumerate(elements):
            ax = axes[i]
            data = results[elem]
            hist = data['cv_history']
            
            x = hist['components']
            y = hist['scores'] # RMSE
            opt_n = data.get('n_components', 0)
            
            # ç»˜åˆ¶æ›²çº¿
            ax.plot(x, y, 'b.-', alpha=0.7, linewidth=1)
            
            # æ ‡è®°é€‰å®šçš„ç‚¹
            if opt_n in x:
                idx = x.index(opt_n)
                ax.plot(x[idx], y[idx], 'ro', markersize=6, label=f'Selected: {opt_n}')
            
            ax.set_title(f"{elem} (n={opt_n})")
            ax.set_xlabel("Components")
            ax.set_ylabel("CV RMSE")
            ax.grid(True, alpha=0.3)
            
        # éšè—å¤šä½™çš„å­å›¾
        for j in range(n_elems, len(axes)):
            axes[j].axis('off')
            
        plt.tight_layout()
        plt.suptitle(f"CV Optimization Curves - {mode_name}", y=1.02, fontsize=16)
        safe_name = mode_name.replace(" ", "_").replace("-", "_").replace("(", "").replace(")", "")
        plt.savefig(os.path.join(save_dir, f"cv_curves_grid_{safe_name}.png"), bbox_inches='tight')
        plt.close()
        
        # --- 2. ç»¼åˆå¯¹æ¯”å›¾ (Combined Plot) ---
        # ä¸ºäº†åœ¨åŒä¸€åæ ‡è½´æ˜¾ç¤ºï¼Œæˆ‘ä»¬å°† RMSE å½’ä¸€åŒ–åˆ° [0, 1] åŒºé—´
        # è¿™æ ·å¯ä»¥æ¯”è¾ƒä¸åŒå…ƒç´ çš„æ”¶æ•›è¶‹åŠ¿
        fig2, ax2 = plt.subplots(figsize=(14, 8), dpi=300)
        
        # ä½¿ç”¨ Tab20 é¢œè‰²è¡¨ä»¥æ”¯æŒæ›´å¤šå…ƒç´ åŒºåˆ† (æœ€å¤š20ç§é¢œè‰²å¾ªç¯)
        cmap = plt.get_cmap('tab20')
        colors = cmap(np.linspace(0, 1, len(elements)))
        
        for idx, elem in enumerate(elements):
            data = results[elem]
            hist = data['cv_history']
            x = np.array(hist['components'])
            y = np.array(hist['scores'])
            opt_n = data.get('n_components', 0)
            
            # è¿‡æ»¤æ— æ•ˆå€¼ (inf/nan)
            mask = np.isfinite(y)
            if not np.any(mask): continue
            
            x_plot = x[mask]
            y_plot = y[mask]
            
            # å½’ä¸€åŒ–: (y - min) / (max - min)
            y_min, y_max = np.min(y_plot), np.max(y_plot)
            if y_max > y_min:
                y_norm = (y_plot - y_min) / (y_max - y_min)
            else:
                y_norm = np.zeros_like(y_plot)
            
            # ç»˜å›¾ (è‡ªåŠ¨é¢œè‰²å¾ªç¯)
            ax2.plot(x_plot, y_norm, '.-', alpha=0.7, linewidth=1.5, label=f"{elem} (n={opt_n})", color=colors[idx])
            
            # æ ‡è®°æœ€ä½³ç‚¹
            if opt_n in x_plot:
                opt_idx = np.where(x_plot == opt_n)[0][0]
                # ä½¿ç”¨å¯¹åº”é¢œè‰²ç»˜åˆ¶æ˜Ÿå·
                ax2.plot(x_plot[opt_idx], y_norm[opt_idx], '*', color=colors[idx], markersize=12, markeredgecolor='white', markeredgewidth=1, zorder=10)

        ax2.set_xlabel("Number of Components", fontsize=12)
        ax2.set_ylabel("Normalized CV RMSE (0=Best, 1=Worst)", fontsize=12)
        ax2.set_title(f"CV Optimization Trends (Normalized) - {mode_name}", fontsize=14, fontweight='bold')
        # å°†å›¾ä¾‹æ”¾åœ¨å›¾å¤–ï¼Œé˜²æ­¢é®æŒ¡
        ax2.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0., fontsize=10)
        ax2.grid(True, alpha=0.3)
        plt.tight_layout()
        
        plt.savefig(os.path.join(save_dir, f"cv_curves_combined_{safe_name}.png"), bbox_inches='tight')
        plt.close()
        
        count += 1
        
    if count > 0:
        print(f"   [Plot] CV å¯»ä¼˜æ›²çº¿å›¾å·²ä¿å­˜è‡³: {save_dir} (å…± {count} ç»„)")
    else:
        print(f"   [Plot] âš ï¸ æœªç”Ÿæˆ CV æ›²çº¿å›¾ (å¯èƒ½æ˜¯å› ä¸ºç»“æœä¸­ç¼ºå°‘ cv_history æ•°æ®)")

def main():
    print("="*80)
    print("      LIBS å…‰è°±æ ¡å‡†ä¸å…ƒç´ é¢„æµ‹ç³»ç»Ÿ (å®Œæ•´æ¶æ„ç‰ˆ)")
    print("="*80)
    
    # 1. åŠ è½½é…ç½®
    config = load_config()
    if config is None:
        return

    # é…ç½®è·¯å¾„ (ä» config è¯»å–)
    base_dir = config['paths']['base_dir']
    lq_dir = os.path.join(base_dir, config['paths']['lq_dir_name'])
    hq_dir = os.path.join(base_dir, config['paths']['hq_dir_name'])
    element_file_path = os.path.join(base_dir, config['paths']['element_file_name'])
    output_dir_name = config['paths'].get('output_dir', 'results')
    
    # åˆ›å»ºç»“æœç›®å½•
    timestamp_dir = create_timestamp_directory(output_dir_name)
    print(f"ğŸ“ ç»“æœè¾“å‡ºç›®å½•: {timestamp_dir}")

    # ä¿å­˜æœ¬æ¬¡è¿è¡Œçš„é…ç½®å¿«ç…§
    config_save_path = os.path.join(timestamp_dir, "config_snapshot.json")
    with open(config_save_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    print(f"   [Config] é…ç½®å¿«ç…§å·²ä¿å­˜: {config_save_path}")

    # 2. æ•°æ®åŠ è½½ (è‡ªåŠ¨æ‰«ææ–‡ä»¶)
    print("\n[Step 1] æ•°æ®åŠ è½½ä¸å¯¹é½...")
    if not os.path.exists(lq_dir):
        print(f"âŒ é”™è¯¯: ç›®å½•ä¸å­˜åœ¨ {lq_dir}")
        return

    sample_files = [f for f in os.listdir(lq_dir) if f.endswith('.csv')]
    sample_ids = sorted([os.path.splitext(f)[0] for f in sample_files])
    print(f"   æ£€æµ‹åˆ° {len(sample_ids)} ä¸ªæ ·å“æ–‡ä»¶")

    # åŠ è½½åŸå§‹æ•°æ®
    lq_raw, hq_raw, lq_wl_raw, wl_common, valid_ids = load_spectral_data_from_csv(lq_dir, hq_dir, sample_ids)
    sample_ids = valid_ids # æ›´æ–°ä¸ºå®é™…åŠ è½½æˆåŠŸçš„æ ·å“IDåˆ—è¡¨
    print(f"   æˆåŠŸåŠ è½½ {len(sample_ids)} å¯¹æœ‰æ•ˆæ•°æ®")

    # ä½¿ç”¨è‡ªåŠ¨è¯†åˆ«çš„å…¬å…±æ³¢é•¿èŒƒå›´ (æ— éœ€æ‰‹åŠ¨è£å‰ª)
    hq_wl_trim = wl_common
    hq_trim = hq_raw
    
    # --- æ™ºèƒ½é‡é‡‡æ ·ç­–ç•¥é€‰æ‹© ---
    config_method = config['preprocessing'].get('resampling_method', 'cubic_spline')
    
    candidates = []
    if isinstance(config_method, list):
        candidates = config_method
    elif config_method == "auto":
        candidates = ["cubic_spline", "pchip", "akima", "linear"]
    else:
        candidates = [str(config_method)]
    
    if not candidates: candidates = ["cubic_spline"]

    print(f"   [Resampling] å‡†å¤‡æ‰§è¡Œé‡é‡‡æ · (é…ç½®æ¨¡å¼: {config_method})...")
    
    best_method = candidates[0]
    best_metrics = {}
    best_rmse = float('inf')
    eval_idx = 0 # é€‰å–ç¬¬ä¸€ä¸ªæ ·å“è¿›è¡Œè¯„ä¼°

    if len(candidates) > 1:
        print(f"   [Auto-Selection] æ­£åœ¨å¯¹æ¯” {len(candidates)} ç§é‡é‡‡æ ·ç®—æ³•...")
        print(f"   {'Method':<15} | {'CV-RMSE':<10} | {'CV-MAPE':<10}")
        print("-" * 45)
        
        for method in candidates:
            metrics = evaluate_resampling_reliability(lq_wl_raw, lq_raw[eval_idx], method=method)
            print(f"   {method:<15} | {metrics['cv_rmse']:.4f}     | {metrics['cv_mape']:.2f}%")
            
            if metrics['cv_rmse'] < best_rmse:
                best_rmse = metrics['cv_rmse']
                best_method = method
                best_metrics = metrics
        print("-" * 45)
        print(f"   âœ… è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹æ³•: {best_method} (RMSE={best_rmse:.4f})")
    else:
        best_method = candidates[0]
        print(f"   [Manual Mode] å·²é”å®šé‡é‡‡æ ·æ–¹æ³•: {best_method} (æ­£åœ¨è¯„ä¼°ä¿çœŸåº¦...)")
        best_metrics = evaluate_resampling_reliability(lq_wl_raw, lq_raw[eval_idx], method=best_method)
        print(f"      CV-RMSE: {best_metrics['cv_rmse']:.4f}, CV-MAPE: {best_metrics['cv_mape']:.2f}%")

    # ä½¿ç”¨é€‰å®šçš„æœ€ä½³æ–¹æ³•è¿›è¡Œå…¨é‡é‡é‡‡æ ·
    print(f"   æ­£åœ¨æ‰§è¡Œ {best_method} é‡é‡‡æ · (LQ -> HQ)...")
    lq_resampled, _ = resample_to_reference(lq_raw, lq_wl_raw, hq_wl_trim, method=best_method)
    
    # --- å¯è§†åŒ–ï¼šåŸå§‹ vs è£å‰ª vs é‡é‡‡æ · ---
    print("   [Visualization] ç”Ÿæˆæ•°æ®åŠ è½½ä¸å¯¹é½å›¾...")
    mask_trim = (lq_wl_raw >= np.min(wl_common)) & (lq_wl_raw <= np.max(wl_common))
    lq_wl_trim = lq_wl_raw[mask_trim]
    lq_spec_trim = lq_raw[eval_idx][mask_trim]
    
    visualize_data_alignment(
        lq_wl_raw, lq_raw[eval_idx],
        lq_wl_trim, lq_spec_trim,
        hq_wl_trim, lq_resampled[eval_idx],
        sample_idx=eval_idx,
        output_dir=os.path.join(timestamp_dir, "data_alignment")
    )
    
    # --- ç”Ÿæˆè¯„ä¼°å›¾ ---
    visualize_resampling_quality(
        orig_wl=lq_wl_raw, orig_spec=lq_raw[eval_idx],
        resamp_wl=hq_wl_trim, resamp_spec=lq_resampled[eval_idx],
        metrics=best_metrics, sample_idx=eval_idx,
        output_dir=os.path.join(timestamp_dir, "resampling_evaluation")
    )
    
    # HQ å¹³å‡åŒ– (Samples, Replicates, Pixels -> Samples, Pixels)
    if hq_trim.ndim == 3:
        hq_avg = np.mean(hq_trim, axis=1)
    else:
        hq_avg = hq_trim

    # 3. é¢„å¤„ç†ä¸å¯è§†åŒ– (åŠŸèƒ½ F)
    print("\n[Step 2] é¢„å¤„ç†å¯è§†åŒ–åˆ†æ...")
    preprocessor = SpectralPreprocessor()
    # å®šä¹‰é¢„å¤„ç†æµ
    steps = config['preprocessing']['steps']
    
    # ä¸ºå¯è§†åŒ–ç”Ÿæˆæ•°æ® (å–ç¬¬ä¸€ä¸ªæ ·å“)
    viz_list = [(lq_resampled[0], "åŸå§‹LQ")]
    temp = lq_resampled[0].copy()
    
    step_viz_dir = os.path.join(timestamp_dir, "step_by_step_preprocessing")
    
    for s in steps:
        method_name = s['method']
        params = s.get('params', {})
        step_name = s['name']
        
        if hasattr(preprocessor, method_name):
            prev_temp = temp.copy()
            # åŠ¨æ€è°ƒç”¨é¢„å¤„ç†æ–¹æ³•ï¼Œæ”¯æŒ config.json ä¸­å®šä¹‰çš„æ‰€æœ‰æ–¹æ³•
            temp = getattr(preprocessor, method_name)(temp, **params)
            
            # è®°å½•åˆ°æ€»åˆ—è¡¨
            viz_list.append((temp, step_name))
            
            # ç”Ÿæˆå•æ­¥å¯¹æ¯”å›¾ (Before vs After)
            visualize_preprocessing_step(
                original_spectrum=prev_temp,
                processed_spectrum=temp,
                wavelengths=hq_wl_trim,
                step_name=step_name,
                sample_idx=0,
                output_dir=step_viz_dir
            )
        else:
            print(f"âš ï¸ è­¦å‘Š: æœªçŸ¥é¢„å¤„ç†æ–¹æ³• {method_name}ï¼Œè·³è¿‡å¯è§†åŒ–ã€‚")
        
    visualize_complete_preprocessing_pipeline(
        viz_list, hq_wl_trim, sample_idx=0, 
        output_dir=os.path.join(timestamp_dir, "overall_preprocessing")
    )
    print("   âœ… é¢„å¤„ç†æµæ°´çº¿å›¾åŠå•æ­¥å¯¹æ¯”å›¾å·²ç”Ÿæˆ")

    # 4. æ¨¡å‹è®­ç»ƒä¸å¯»ä¼˜ (åŠŸèƒ½ B)
    print("\n[Step 3] è®­ç»ƒå…‰è°±æ ¡å‡†æ¨¡å‹...")
    
    # 4.0 å…ˆåˆ’åˆ†æ•°æ®é›† (Indices)
    manual_split = config['model'].get('manual_split', {})
    
    if manual_split.get('enabled', False):
        print("   âš ï¸ ä½¿ç”¨æ‰‹åŠ¨æ•°æ®é›†åˆ’åˆ† (Configured in config.json)")
        train_names = manual_split.get('train_samples', [])
        test_names = manual_split.get('test_samples', [])
        
        # å»ºç«‹åç§°åˆ°ç´¢å¼•çš„æ˜ å°„
        name_to_idx = {name: i for i, name in enumerate(sample_ids)}
        all_indices = set(range(len(sample_ids)))
        
        train_idx_set = set()
        val_idx_set = set()
        
        # è§£æé…ç½®ä¸­çš„æ ·å“å
        for name in train_names:
            name_str = str(name)
            if name_str in name_to_idx: train_idx_set.add(name_to_idx[name_str])
            else: print(f"   [Warning] è®­ç»ƒé›†æ ·å“æœªæ‰¾åˆ°: {name}")
                
        for name in test_names:
            name_str = str(name)
            if name_str in name_to_idx: val_idx_set.add(name_to_idx[name_str])
            else: print(f"   [Warning] æµ‹è¯•é›†æ ·å“æœªæ‰¾åˆ°: {name}")
        
        # è‡ªåŠ¨è¡¥å…¨é€»è¾‘ (äº’æ–¥è¡¥å…¨)
        if train_idx_set and not val_idx_set:
            val_idx_set = all_indices - train_idx_set
            print(f"   è‡ªåŠ¨åˆ†é…å‰©ä½™ {len(val_idx_set)} ä¸ªæ ·å“åˆ°æµ‹è¯•é›†")
        elif val_idx_set and not train_idx_set:
            train_idx_set = all_indices - val_idx_set
            print(f"   è‡ªåŠ¨åˆ†é…å‰©ä½™ {len(train_idx_set)} ä¸ªæ ·å“åˆ°è®­ç»ƒé›†")
            
        if not train_idx_set or not val_idx_set:
            raise ValueError("æ‰‹åŠ¨åˆ’åˆ†é…ç½®é”™è¯¯ï¼šè®­ç»ƒé›†æˆ–æµ‹è¯•é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ config.json ä¸­çš„æ ·å“åç§°ã€‚")
            
        train_idx = sorted(list(train_idx_set))
        val_idx = sorted(list(val_idx_set))
    else:
        test_size = config['model'].get('test_size', 0.2)
        random_state = config['model'].get('random_state', 42)
        # ä»…åˆ’åˆ†ç´¢å¼•
        train_idx, val_idx = train_test_split(range(len(lq_resampled)), test_size=test_size, random_state=random_state)

    # æå–åˆå§‹æ•°æ® (Raw)
    train_lq = lq_resampled[train_idx].copy()
    val_lq = lq_resampled[val_idx].copy()
    train_hq = hq_avg[train_idx].copy()
    val_hq = hq_avg[val_idx].copy()

    # 4.1 å†åº”ç”¨é¢„å¤„ç† (åˆ†åˆ«å¤„ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œé˜²æ­¢æ³„æ¼)
    for s in steps:
        method_name = s['method']
        params = s.get('params', {})
        if hasattr(preprocessor, method_name):
            print(f"   æ‰§è¡Œé¢„å¤„ç†: {s['name']} (Train/Val åˆ†ç¦»å¤„ç†)")
            train_lq = getattr(preprocessor, method_name)(train_lq, **params)
            val_lq = getattr(preprocessor, method_name)(val_lq, **params)
            train_hq = getattr(preprocessor, method_name)(train_hq, **params)
            val_hq = getattr(preprocessor, method_name)(val_hq, **params)

    # 4.2 é‡ç»„å…¨é‡æ•°æ® (ç”¨äºåç»­ Pipeline)
    lq_proc = np.zeros((len(lq_resampled), train_lq.shape[1]))
    lq_proc[train_idx] = train_lq
    lq_proc[val_idx] = val_lq
    
    hq_proc = np.zeros((len(hq_avg), train_hq.shape[1]))
    hq_proc[train_idx] = train_hq
    hq_proc[val_idx] = val_hq

    # 4.1 è‡ªåŠ¨å¯»ä¼˜
    print("   >>> æ­£åœ¨è®­ç»ƒå…‰è°±æ ¡å‡†æ¨¡å‹...")
    
    # è·å–æ ¡å‡†é…ç½®
    calib_config = config['model'].get('calibration', {})
    # å…¼å®¹æ—§é…ç½® (å¦‚æœ calibration ä¸å­˜åœ¨)
    if not calib_config:
        calib_config = {
            "method": "PLSR",
            "params": config['model'] # å°è¯•ä½¿ç”¨é¡¶å±‚å‚æ•°
        }
        
    calib_params = calib_config.get('params', {})
    learn_diff = calib_params.get('learn_difference', False)
    feature_selection_config = config['model'].get('feature_selection', {"enabled": False})
    mode_strategies = config['model'].get('mode_strategies', {})
    mode_fs_configs = config['model'].get('mode_feature_selection', {})
    
    if learn_diff:
        print("   [Strategy] å¯ç”¨å·®å¼‚å­¦ä¹  (Difference Learning: HQ - LQ)...")
        train_target = train_hq - train_lq
        X_base_for_cv = train_lq
    else:
        print("   [Strategy] æ ‡å‡†ç›´æ¥å­¦ä¹  (Direct Learning: HQ)...")
        train_target = train_hq
        X_base_for_cv = None
        
    # è°ƒç”¨é€šç”¨è®­ç»ƒå‡½æ•°
    calib_model, calib_metrics = train_calibration_model(
        train_lq, train_target, 
        config=calib_config, 
        timestamp_dir=timestamp_dir, 
        X_base=X_base_for_cv
    )
    
    print(f"   âœ… æ ¡å‡†æ¨¡å‹è®­ç»ƒå®Œæˆ (Params: {calib_metrics.get('n_components', 'N/A')}, Score: {calib_metrics.get('score', 0):.4f})")
    
    # 4.3 è¯„ä¼°
    if learn_diff:
        val_diff_pred = calib_model.predict(val_lq)
        val_pred = val_lq + val_diff_pred # é‡æ„: LQ + Predicted_Diff
    else:
        val_pred = calib_model.predict(val_lq)
        
    print(f"   å…‰è°±æ ¡å‡†éªŒè¯é›† RÂ²: {r2_score(val_hq.flatten(), val_pred.flatten()):.4f}")
    plot_results(val_lq, val_hq, val_pred, hq_wl_trim, sample_idx=0, title=f"æ ¡å‡†æ•ˆæœ ({'Diff' if learn_diff else 'Direct'})", timestamp_dir=timestamp_dir)

    # 5. å¤šæ¨¡å¼å…ƒç´ é¢„æµ‹ (åŠŸèƒ½ C)
    print("\n[Step 4] æ‰§è¡Œå…ƒç´ é¢„æµ‹ (LQ-only vs Calib-Spec vs HQ-only)...")
    element_data = load_element_data(element_file_path)
    if element_data is None: return

    # --- å…³é”®ä¿®å¤ï¼šæ‰§è¡Œæ•°æ®å¯¹é½ ---
    element_data = align_element_data(element_data, sample_ids)

    # è·å–é¢„æµ‹é…ç½®
    pred_config = config['model'].get('prediction', {})
    if not pred_config:
        pred_config = {"method": "PLSR", "params": config['model']}

    pipeline = ElementPredictionPipeline(
        spectral_model=calib_model, 
        prediction_config=pred_config,
        feature_selection_config=feature_selection_config, 
        wavelengths=hq_wl_trim
    )
    
    # æ¨¡å¼1: LQ-only
    print("\n   [Mode 1] LQ-only (åŸºå‡†)")
    strat = mode_strategies.get('LQ-only', None)
    fs_cfg = mode_fs_configs.get('LQ-only', feature_selection_config)
    res_lq = pipeline.train_element_models_with_lq_only(lq_proc, element_data, train_idx, val_idx, timestamp_dir, selection_method=strat, feature_selection_config=fs_cfg)
    
    # æ¨¡å¼2: Calib-Spec
    print("\n   [Mode 2] Calib-Spec (æ ¸å¿ƒ: Train on HQ, Test on Calib-LQ)")
    strat = mode_strategies.get('Calib-Spec', None)
    # ç”Ÿæˆå…¨é‡æ ¡å‡†å…‰è°±
    if learn_diff:
        lq_calibrated_diff = calib_model.predict(lq_proc)
        lq_calibrated = lq_proc + lq_calibrated_diff
    else:
        lq_calibrated = calib_model.predict(lq_proc)
        
    fs_cfg = mode_fs_configs.get('Calib-Spec', feature_selection_config)
    res_calib = pipeline.train_element_models_hq_train_calib_test(hq_proc, lq_calibrated, element_data, train_idx, val_idx, timestamp_dir, selection_method=strat, feature_selection_config=fs_cfg)
    
    # æ¨¡å¼3: HQ-only
    print("\n   [Mode 3] HQ-only (ä¸Šé™)")
    strat = mode_strategies.get('HQ-only', None)
    fs_cfg = mode_fs_configs.get('HQ-only', feature_selection_config)
    res_hq = pipeline.train_element_models_with_hq_only(hq_proc, element_data, train_idx, val_idx, timestamp_dir, selection_method=strat, feature_selection_config=fs_cfg)

    # æ¨¡å¼4: Calib-Self (å®ç”¨æ¨¡å¼)
    print("\n   [Mode 4] Calib-Self (å®ç”¨: Train on Calib-LQ, Test on Calib-LQ)")
    strat = mode_strategies.get('Calib-Self', None)
    fs_cfg = mode_fs_configs.get('Calib-Self', feature_selection_config)
    res_calib_self = pipeline.train_element_models_with_calibrated_spectra(lq_calibrated, element_data, train_idx, val_idx, timestamp_dir, selection_method=strat, feature_selection_config=fs_cfg)

    # 5.1 ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    print("\n[Step 5] ç”Ÿæˆç»¼åˆå¯¹æ¯”åˆ†æå›¾...")
    plot_performance_comparison(res_lq, res_calib, res_hq, timestamp_dir, res_calib_self)
    plot_component_counts(res_lq, res_calib, res_hq, timestamp_dir, res_calib_self)
    plot_cv_curves(res_lq, res_calib, res_hq, timestamp_dir, res_calib_self)
    
    # å®‰å…¨è®¡ç®—å…±åŒå…ƒç´  (é˜²æ­¢ res_calib_self ä¸º None)
    keys_sets = [set(res_lq.keys()), set(res_calib.keys()), set(res_hq.keys())]
    if res_calib_self:
        keys_sets.append(set(res_calib_self.keys()))
    common_elements = set.intersection(*keys_sets)
    
    for elem in common_elements:
        plot_prediction_scatter_comparison(res_lq, res_calib, res_hq, elem, timestamp_dir, res_calib_self)

    # 6. æ‰“å°æ€»ç»“
    print("\n[Summary] å…³é”®å…ƒç´  (SiO2) RÂ² å¯¹æ¯”:")
    # å°è¯•æŸ¥æ‰¾ SiO2 (å¤„ç†å¯èƒ½çš„ç©ºæ ¼æˆ–å‘½åå·®å¼‚)
    sio2_candidates = ['SiO2', 'SiO2 ', 'Si', 'Si ']
    target_elem = next((e for e in sio2_candidates if e in res_lq), None)
    
    if target_elem:
        print(f"   Element : {target_elem}")
        print(f"   LQ-only : {res_lq[target_elem]['r2']:.4f}")
        print(f"   Calib   : {res_calib[target_elem]['r2']:.4f} (Mode 2)")
        if res_calib_self and target_elem in res_calib_self:
            print(f"   Calib-S : {res_calib_self[target_elem]['r2']:.4f} (Mode 4)")
        print(f"   HQ-only : {res_hq[target_elem]['r2']:.4f}")
    else:
        print("   (æœªæ‰¾åˆ° SiO2 ç›¸å…³å…ƒç´ ï¼Œè·³è¿‡å±•ç¤º)")
        
    print(f"\nâœ… å®Œæ•´æµç¨‹ç»“æŸï¼è¯·æŸ¥çœ‹ç»“æœæ–‡ä»¶å¤¹: {timestamp_dir}")

if __name__ == "__main__":
    main()